## Overview od Data Pipelines 

1. **What is a Data Pipeline?**
   - Definition: An automated process that extracts data from a source system, transforms it into a desired model, and loads the data into a file, database, or other data storage tool
   - ETL Pipelines: Extract, Transform, Load.
   - Automation is a key feature, triggered by events or schedules.

2. **Data Pipeline Users**
   - Users: Data Analysts, Data Scientists, ML Engineers, Business Intelligence Analysts.
   - Dependency: Data pipelines can be used by other data pipelines.
   - Utilization: Data supports analytics, dashboards, ML model training, ad hoc analyses, etc.

3. **Building Data Pipelines**
   - Builders: Data Engineers (ETL Developers).
   - Tools: Python and SQL as foundational, Apache Airflow for orchestration.
   - Collaboration: Data Architects work with Data Engineers.
   - Emphasis: Use Python and SQL to create effective and resilient data pipelines.

4. **Components of an ETL Pipeline**
   - Extraction: Pull data from source (file, database, API).
   - Transformation: Convert raw data to desired model.
   - Loading: Load transformed data into storage (file, data warehouse, API).
   - ELT Pipeline: Briefly mentioned as an alternative architecture, but focus is on ETL pipelines.
